from google.cloud import bigquery, storage
import pandas as pd
import io
import os
import tempfile
from datetime import datetime
import pytz

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Google Cloud
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/opt/airflow/dags/keys/gcp.json"

# ============================================================
# ğŸ”¹ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙÙŠ BigQuery
# ============================================================
def ensure_table_exists(client, table_id, schema):
    """ÙŠØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø£Ùˆ ÙŠÙ†Ø´Ø¦Ù‡ ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯Ù‡."""
    try:
        client.get_table(table_id)
        print(f"âœ… Table {table_id} already exists.")
    except Exception:
        table = bigquery.Table(table_id, schema=schema)
        client.create_table(table)
        print(f"ğŸ†• Created new table: {table_id}")

# ============================================================
# ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ BigQuery
# ============================================================
def load_to_bq(df):
    """Loads currency data into BigQuery (append + merge)."""
    client = bigquery.Client()
    project = client.project
    dataset = "currency"

    historical_table = f"{project}.{dataset}.historical_rates"
    current_table = f"{project}.{dataset}.current_rates"
    tmp_table = f"{project}.{dataset}.tmp_rates"

    print("ğŸš€ Starting BigQuery load process...")

    # ğŸ§¹ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    if "target_currency" not in df.columns and "pair" in df.columns:
        df[["base_currency", "target_currency"]] = df["pair"].str.extract(r"([A-Z]{3})([A-Z]{3})")

    df["rate"] = pd.to_numeric(df["rate"], errors="coerce")
    df["base_currency"] = df["base_currency"].astype(str)
    df["target_currency"] = df["target_currency"].astype(str)

    # âœ… Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„Ù…Ø­Ù„ÙŠ
    cairo_tz = pytz.timezone("Africa/Cairo")
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
    df["retrieved_at"] = datetime.now(cairo_tz)

    # âœ… ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    df = df[["base_currency", "target_currency", "rate", "timestamp", "retrieved_at"]].dropna()

    # âœ… ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Schema
    schema = [
        bigquery.SchemaField("base_currency", "STRING"),
        bigquery.SchemaField("target_currency", "STRING"),
        bigquery.SchemaField("rate", "FLOAT64"),
        bigquery.SchemaField("timestamp", "TIMESTAMP"),
        bigquery.SchemaField("retrieved_at", "TIMESTAMP"),
    ]

    # âœ… ØªØ£ÙƒÙŠØ¯ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
    ensure_table_exists(client, historical_table, schema)
    ensure_table_exists(client, current_table, schema)
    ensure_table_exists(client, tmp_table, schema)

    # âœ… Ø¥Ù†Ø´Ø§Ø¡ CSV Ù…Ø¤Ù‚Øª
    with tempfile.NamedTemporaryFile(mode="w+", suffix=".csv", delete=False) as tmp_csv:
        df.to_csv(tmp_csv.name, index=False)
        tmp_path = tmp_csv.name
    print(f"ğŸ“„ Temporary CSV created at {tmp_path}")

    # âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ historical_rates
    hist_config = bigquery.LoadJobConfig(
        schema=schema,
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        write_disposition="WRITE_APPEND",
    )
    with open(tmp_path, "rb") as f:
        client.load_table_from_file(f, historical_table, job_config=hist_config).result()
    print("âœ… Appended to historical_rates successfully!")

    # âœ… ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¤Ù‚ØªØ© Ø¥Ù„Ù‰ tmp_rates
    tmp_config = bigquery.LoadJobConfig(
        schema=schema,
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        write_disposition="WRITE_TRUNCATE",
    )
    with open(tmp_path, "rb") as f:
        client.load_table_from_file(f, tmp_table, job_config=tmp_config).result()

    # âœ… MERGE Ù„ØªØ­Ø¯ÙŠØ« current_rates
    merge_sql = f"""
    MERGE `{current_table}` T
    USING `{tmp_table}` S
    ON T.base_currency = S.base_currency AND T.target_currency = S.target_currency
    WHEN MATCHED AND S.timestamp > T.timestamp THEN
      UPDATE SET 
        rate = S.rate,
        timestamp = S.timestamp,
        retrieved_at = S.retrieved_at
    WHEN NOT MATCHED THEN
      INSERT (base_currency, target_currency, rate, timestamp, retrieved_at)
      VALUES (S.base_currency, S.target_currency, S.rate, S.timestamp, S.retrieved_at)
    """
    client.query(merge_sql).result()
    print("âœ… current_rates table merged successfully!")

    # ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
    try:
        client.delete_table(tmp_table, not_found_ok=True)
        os.remove(tmp_path)
    except Exception as e:
        print(f"âš ï¸ Cleanup warning: {e}")
    print("ğŸ§¹ Temporary files and tables cleaned successfully!")

# ============================================================
# ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù…Ø¹ÙŠÙ† Ù…Ù† GCS (Airflow ÙŠÙ…Ø±Ø± Ø§Ù„Ø§Ø³Ù…)
# ============================================================
def load_from_gcs_to_bq(gcs_filename):
    """Reads a specific transformed CSV from GCS and loads it to BigQuery."""
    bucket_name = "bigdata-ai-datalake"
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(gcs_filename)

    print(f"ğŸ“¥ Downloading file from GCS: gs://{bucket_name}/{gcs_filename}")
    if not blob.exists():
        raise FileNotFoundError(f"âŒ File not found in GCS: {gcs_filename}")

    # âœ… Ù‚Ø±Ø§Ø¡Ø© CSV ÙƒÙ€ DataFrame
    data = blob.download_as_bytes()
    df = pd.read_csv(io.BytesIO(data))
    print(f"âœ… File downloaded successfully â€” {len(df)} rows")

    # âœ… ØªØ­Ù…ÙŠÙ„ Ø¥Ù„Ù‰ BigQuery
    load_to_bq(df)
    print("ğŸ¯ File loaded successfully into BigQuery!")

# ============================================================
# ğŸ”¹ Main Entry Point (Ù„Ù„Ù€ local testing ÙÙ‚Ø·)
# ============================================================
if __name__ == "__main__":
    print("ğŸš€ Starting Load Stage to BigQuery (manual test)...")
    test_file = "clean1/exchangerate/live/USD_transformed_20251110_140000.csv"
    try:
        load_from_gcs_to_bq(test_file)
        print("ğŸ‰ BigQuery load pipeline completed successfully (Cairo Time)!")
    except Exception as e:
        print(f"âŒ Error in Load pipeline: {e}")
